{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOcAUfcQl0DrCzyDYSRH3N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Jax implementation of \"A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing\"\n",
        "\n",
        "Paper: https://arxiv.org/abs/2308.06447"
      ],
      "metadata": {
        "id": "ok2OS_HnqLcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import jax\n",
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "from jax import random, grad, vmap, jit\n",
        "\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "from tqdm import trange\n",
        "from scipy.stats import qmc\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "oyThWh2Kbd6i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System specification (Refer to the paper for more details: https://arxiv.org/abs/2308.06447)\n",
        "T_max = 200 + 273 # Max temperature (k)\n",
        "U_min = 0.001 # Initial degree of cure (doc)\n",
        "U_diff = 1 - U_min\n",
        "T_scaler = 1/T_max # Temperature scaler\n",
        "T_min = (20 + 273) * T_scaler # Min temperature (k)\n",
        "t_end = 18800 # Cure cycle duration (s)\n",
        "t_raw = np.linspace(0, t_end, 5000)\n",
        "t_scaler = 1/t_raw.max() # Time sclaer\n",
        "t = t_raw * t_scaler\n",
        "\n",
        "# 2-hold Cure cycle specification (scaled)\n",
        "T_ini = (20 + 273) * T_scaler # Initial temperature (k)\n",
        "ramp_rate_1 = (2/60) * (T_scaler/t_scaler) # Heat rate 1 (K/s)\n",
        "T_hold_1 = (110 + 273) * T_scaler # Hold 1 temperature (k)\n",
        "t_ramp_1_end = (T_hold_1 - T_ini)/ramp_rate_1 # When heat ramp 1 ends (s)\n",
        "t_hold_1_end = 60*60*t_scaler + t_ramp_1_end # When hold 1 ends (s)\n",
        "ramp_rate_2 = (2/60) * (T_scaler/t_scaler) # Heat rate 2 (K/s)\n",
        "T_hold_2 = (180 + 273) * T_scaler # Hold 2 temperature (k)\n",
        "t_ramp_2_end = (T_hold_2 - T_hold_1)/ramp_rate_2 + t_hold_1_end # When heat ramp 2 ends (s)\n",
        "t_hold_2_end = 120*60*t_scaler + t_ramp_2_end # When hold 2 ends (s)\n",
        "T_end = (20 + 273) * T_scaler\n",
        "ramp_rate_3 = (3/60) * (T_scaler/t_scaler) # Cool down heat rate (K/s)\n",
        "t_ramp_3_end = (T_hold_2 - T_end)/ramp_rate_3 + t_hold_2_end # When heat ramp 3 ends (s)\n",
        "\n",
        "# Air temperature function\n",
        "def Temp_air(t):\n",
        "    return np.where(t < t_ramp_1_end, T_ini + t*ramp_rate_1, #  Ramp 1\n",
        "             np.where(t < t_hold_1_end, T_hold_1, # Hold 1\n",
        "                      np.where(t < t_ramp_2_end, T_hold_1 + (t - t_hold_1_end)*ramp_rate_2, # Ramp 2\n",
        "                               np.where(t < t_hold_2_end, T_hold_2, # Hold 2\n",
        "                                       T_hold_2 - (t - t_hold_2_end)*ramp_rate_3))) # Ramp 3\n",
        "             )\n",
        "\n",
        "\n",
        "T_boundary = Temp_air(t)\n",
        "plt.plot(T_boundary)\n",
        "T_mean = (T_boundary/T_scaler).mean() # C\n",
        "\n",
        "# Composite part specifications\n",
        "# Adapted from:\n",
        "# 1) Andrew Johnson's thesis: https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/831/items/1.0088805\n",
        "# 2) Niaki et al. https://www.sciencedirect.com/science/article/pii/S0045782521002966\n",
        "# 3) Raven software: https://www.convergent.ca/products/raven-simulation-software\n",
        "\n",
        "part_len = 0.03 #  part length (m)\n",
        "part_nodes = 50\n",
        "x_raw = np.linspace(0, part_len, part_nodes)\n",
        "x_scaler = 1/x_raw.max() # Scaler\n",
        "x = x_raw*x_scaler\n",
        "\n",
        "x_lb, x_ub = x[1], x[-2]\n",
        "t_lb, t_ub = t[1], t[-2]\n",
        "lb = np.array([x_lb, t_lb])\n",
        "ub = np.array([x_ub, t_ub])\n",
        "\n",
        "X, T = np.meshgrid(x, t) # X: [T, X], T: [T, X]\n",
        "\n",
        "X = X.flatten()[:, None]\n",
        "T = T.flatten()[:, None]\n",
        "\n",
        "F = np.asarray(np.hstack([X, T]))\n",
        "\n",
        "# Model parameters and normalization\n",
        "T_ave = T_mean # Average temperature for properties estimation (k)\n",
        "alpha_ave = np.array(0.5) #  Average doc for properties estimation\n",
        "\n",
        "# Fibre properties\n",
        "rho_f = np.array(1.790e03) # fibre density (kg/m3), RAVEN Model\n",
        "k_f = 2.4  + 1.560e-2 * (T_ave-273) # Fibre thermal conductivity (W/(m K)), RAVEN Model, transverse direction\n",
        "Cp_f = 750 + 2.05 *  (T_ave-273-20)  # Fibre specific heat capacity (J/ (kg K)), RAVEN Model\n",
        "\n",
        "# Resin properties\n",
        "rho_r = np.array(1.300e3) # Resin density (kg/m3), RAVEN Model\n",
        "k_r = 0.148 + 3.430E-04 * (T_ave-273) + 6.070E-02 * alpha_ave # Resin thermal conductivity (W/(m K)), RAVEN Model\n",
        "Cp_r = 1005 + 3.74 *  (T_ave-273-20)  # resin specific heat capacity (J/ (kg K)), RAVEN Model\n",
        "H_r = np.array(5.4e5) # Resin heat of reasction per unit mass (J / kg), RAVEN Model\n",
        "nu_r = np.array(0.426) # Resin volume fraction in composite material (1-0.574)\n",
        "h_c = np.array(120.0) # Heat trasnfer coefficient (W/(m2 K))\n",
        "\n",
        "# Cure kinetics properties, 8552 epoxy resin\n",
        "A = np.array(1.528e5)   # Pre-exponential cure rate coefficient (1/s)\n",
        "dE = np.array(6.650e4)     #  Activation energy (J/mol)\n",
        "M = np.array(0.8129) # First exponential constant\n",
        "N = np.array(2.7360) # Second exponential constant\n",
        "C = np.array(43.09) # Diffusion constant\n",
        "ALCT = np.array(5.475e-3) #  Constant accounting for increase in critical resin degree of cure with temperature (1/K)\n",
        "ALC = np.array(-1.6840) # Critical degree of cure at T = 0 K\n",
        "R = np.array(8.314)     # Gas constant (J/(mol K))\n",
        "\n",
        "# Composite part properties (fibre+resin)\n",
        "nu_f = 1. - nu_r # fiber volume fraction\n",
        "rho_c = rho_r * nu_r + rho_f * nu_f # Density (kg/m3)\n",
        "Cp_c = Cp_r * nu_r + Cp_f * nu_f # Specific heat capacity (J/ (kg K))\n",
        "BB = 2 * (k_r/k_f - 1) #eq (B.70) Andrew Johnston thesis\n",
        "CC = (nu_f/math.pi)**0.5  #eq (B.70) Andrew Johnston thesis\n",
        "DD = (1-(BB**2)*(CC**2))**0.5  #eq (B.70) Andrew Johnston thesis\n",
        "k_c = k_r *( (1-2*CC) + 1 / BB * (math.pi-4/DD*math.atan(DD/(1+BB*CC))) )# Thermal conductivity (W/(m K)) eq (B.70) Andrew Johnston thesis\n",
        "a_c =  k_c/(rho_c*Cp_c) # a in heat trasnfer PDE (m2 / s)\n",
        "b =  rho_r*H_r*nu_r/(rho_c*Cp_c) # b in heat trasnfer PDE (K)\n",
        "b = b/1.8255 # Correction\n",
        "\n",
        "# Normalized properties\n",
        "a_c_norm = a_c * ((x_scaler)**2)/t_scaler\n",
        "b_norm = b * T_scaler\n",
        "A_norm = A/t_scaler\n",
        "dE_norm = dE * x_scaler**2/t_scaler**2\n",
        "R_norm = R * (x_scaler**2)/(t_scaler**2) /T_scaler\n",
        "ALCT_norm = ALCT /T_scaler"
      ],
      "metadata": {
        "id": "Fkf2kjJ8bd9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network class\n",
        "class Net(nn.Module):\n",
        "    features: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=64, name='trainable1')(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=64, name='trainable2')(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=64, name='trainable3')(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=64, name='trainable4')(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=2, name='trainable5')(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3akaB83Hbd_v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define necessary functions to construct PINNs loss function\n",
        "\n",
        "# Neural network with 2 output neurons (T and doc)\n",
        "def neural_net(params, x, t):\n",
        "\n",
        "    z = np.hstack([x, t])\n",
        "    outputs = model.apply(params, z)\n",
        "    return outputs[0]\n",
        "\n",
        "# Function outputting corrected T neuron value\n",
        "def neural_net_T(params, x, t):\n",
        "\n",
        "    z = np.hstack([x, t])\n",
        "    outputs = model.apply(params, z)\n",
        "    T, a = np.split(outputs, 2)\n",
        "    T_corrected = jax.nn.softplus(T[0]) + T_min\n",
        "\n",
        "    return T_corrected\n",
        "\n",
        "# Function outputting corrected doc neuron value\n",
        "def neural_net_a(params, x, t):\n",
        "\n",
        "    z = np.hstack([x, t])\n",
        "    outputs = model.apply(params, z)\n",
        "    T, a = np.split(outputs, 2)\n",
        "    a_corrected = U_diff*jax.nn.sigmoid(a[0]) + U_min\n",
        "\n",
        "    return a_corrected\n",
        "\n",
        "# Cure kinetics (Andrew Johnson thesis)\n",
        "def cure_kinetics(params, x, t):\n",
        "\n",
        "    T = neural_net_T(params, x, t)\n",
        "    alpha = neural_net_a(params, x, t)\n",
        "    num = A_norm * np.exp(-dE_norm/(R_norm*T))\n",
        "    den = 1. + np.exp(C*(alpha - (ALC + ALCT_norm*T)))\n",
        "    cure_term = (num/den)*(alpha**M)*((1-alpha)**N)\n",
        "\n",
        "    return cure_term\n",
        "\n",
        "# ODE loss\n",
        "def ode_net(params, x, t):\n",
        "\n",
        "    cure_term = cure_kinetics(params, x, t)\n",
        "    a_t = grad(neural_net_a, argnums=2)(params, x, t)\n",
        "    f = a_t - cure_term\n",
        "\n",
        "    return f\n",
        "\n",
        "# PDE loss calculator\n",
        "def pde_net(params, x, t):\n",
        "\n",
        "    cure_term = cure_kinetics(params, x, t)\n",
        "    T_t = grad(neural_net_T, argnums=2)(params, x, t)\n",
        "    T_xx = grad(grad(neural_net_T, argnums = 1), argnums=1)(params, x, t)\n",
        "    f = T_t - a_c_norm*T_xx - b_norm*cure_term\n",
        "\n",
        "    return f\n",
        "\n",
        "# Bottom BC loss calculator\n",
        "def bcb_net(params, C_bot, x, t):\n",
        "\n",
        "    u_bot = neural_net_T(params, x, t)\n",
        "    T_air = Temp_air(t)\n",
        "    T_x = grad(neural_net_T, argnums=1)(params, x, t)\n",
        "    f = (T_air - u_bot) + C_bot*T_x\n",
        "\n",
        "    return f\n",
        "\n",
        "# Top BC loss calculator\n",
        "def bct_net(params, C_top, x, t):\n",
        "\n",
        "    u_top = neural_net_T(params, x, t)\n",
        "    T_air = Temp_air(t)\n",
        "    T_x = grad(neural_net_T, argnums=1)(params, x, t)\n",
        "    f = (u_top - T_air) + C_top*T_x\n",
        "\n",
        "    return f\n",
        "\n",
        "# vmap the functions\n",
        "u_pred_fn = vmap(neural_net_T, (None, 0, 0))\n",
        "u_pred_fn_a = vmap(neural_net_a, (None, 0, 0))\n",
        "p_pred_fn = vmap(pde_net, (None, 0, 0))\n",
        "o_pred_fn = vmap(ode_net, (None, 0, 0))\n",
        "cure_fn = vmap(cure_kinetics, (None, 0, 0))\n",
        "bcb_fn = vmap(bcb_net, (None, None, 0, 0))\n",
        "bct_fn = vmap(bct_net, (None, None, 0, 0))\n",
        "\n",
        "# Evaluate the network and the residual over the grid\n",
        "@jit\n",
        "def loss_pde(params, F_pde_intv):\n",
        "\n",
        "    p_pred = p_pred_fn(params, F_pde_intv[:,0], F_pde_intv[:,1])\n",
        "    # Compute loss\n",
        "    loss_p = np.mean(p_pred**2)\n",
        "\n",
        "    return loss_p\n",
        "\n",
        "@jit\n",
        "def loss_ode(params, F_pde_intv):\n",
        "\n",
        "    o_pred = o_pred_fn(params, F_pde_intv[:,0], F_pde_intv[:,1])\n",
        "    # Compute loss\n",
        "    loss_o = np.mean(o_pred**2)\n",
        "\n",
        "    return loss_o\n",
        "\n",
        "@jit\n",
        "def loss_ics_T(params, F_ic_intv, U_T_ic_intv):\n",
        "    # Evaluate the network over IC\n",
        "    u_pred = vmap(neural_net_T, (None, 0, 0))(params, F_ic_intv[:,0], F_ic_intv[:,1])\n",
        "    # Compute the initial loss\n",
        "    loss_ics = np.mean((U_T_ic_intv.flatten() - u_pred.flatten())**2)\n",
        "\n",
        "    return loss_ics\n",
        "\n",
        "@jit\n",
        "def loss_ics_a(params, F_ic_intv, U_a_ic_intv):\n",
        "    # Evaluate the network over IC\n",
        "    u_pred = vmap(neural_net_a, (None, 0, 0))(params, F_ic_intv[:,0], F_ic_intv[:,1])\n",
        "    # Compute the initial loss\n",
        "    loss_ics = np.mean((U_a_ic_intv.flatten() - u_pred.flatten())**2)\n",
        "\n",
        "    return loss_ics\n",
        "\n",
        "@jit\n",
        "def loss_bcb(params, C_bot, F_bcb_intv):\n",
        "\n",
        "    p_pred = bcb_fn(params, C_bot, F_bcb_intv[:,0], F_bcb_intv[:,1])\n",
        "    loss_p = np.mean(p_pred**2)\n",
        "\n",
        "    return loss_p\n",
        "\n",
        "@jit\n",
        "def loss_bct(params, C_top, F_bct_intv):\n",
        "\n",
        "    p_pred = bct_fn(params, C_top, F_bct_intv[:,0], F_bct_intv[:,1])\n",
        "    loss_p = np.mean(p_pred**2)\n",
        "\n",
        "    return loss_p\n",
        "\n",
        "# PINNs loss function\n",
        "@jit\n",
        "def loss(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde):\n",
        "\n",
        "    L_ic_T = loss_ics_T(params, F_ic_intv, U_T_ic_intv)\n",
        "    L_ic_a = loss_ics_a(params, F_ic_intv, U_a_ic_intv)\n",
        "    L_bcb = loss_bcb(params, C_bot, F_bcb_intv)\n",
        "    L_bct = loss_bct(params, C_top, F_bct_intv)\n",
        "    L_pde = loss_pde(params, F_pde_intv)\n",
        "    L_ode = loss_ode(params, F_pde_intv)\n",
        "    # Compute loss\n",
        "    loss = w_ic_T*L_ic_T + w_ic_a*L_ic_a + w_bcb*L_bcb + w_bct*L_bct + w_pde*L_ode + w_pde*L_pde\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Vmap the loss components\n",
        "loss_ics_T_vmap = vmap(loss_ics_T, (None, None, 0))\n",
        "loss_ics_a_vmap = vmap(loss_ics_a, (None, None, 0))\n",
        "loss_bcb_vmap = vmap(loss_bcb, (None, 0, None))\n",
        "loss_bct_vmap = vmap(loss_bct, (None, 0, None))\n",
        "loss_vmap = vmap(loss, (None, 0, 0, None, None, None, None, 0, 0, None, None, None, None, None))"
      ],
      "metadata": {
        "id": "BxxeBNhFbeCE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta-learning (based on MAML formulation: https://arxiv.org/abs/1703.03400)\n",
        "\n",
        "@jit\n",
        "def update_params(params, grads, learning_rate = 0.00001):\n",
        "\n",
        "  params = jax.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
        "\n",
        "  return params\n",
        "\n",
        "# Inner loop optimization (see MAML paper)\n",
        "@jit\n",
        "def step_inner(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "               F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test):\n",
        "\n",
        "    grads = grad(loss, argnums=0)(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde)\n",
        "    params = update_params(params, grads)\n",
        "    loss_value = loss(params, C_bot, C_top, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "# Vmap the inner loop\n",
        "step_inner_vmap = vmap(step_inner, (None, 0, 0, None, None, None, None, 0, 0, None, None, None, None, None, None, None, None, None, 0, 0))\n",
        "\n",
        "# Calculate average loss among support set tasks\n",
        "@jit\n",
        "def step_inner_loss_mean(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "               F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test):\n",
        "\n",
        "    loss_all = step_inner_vmap(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "                   F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test)\n",
        "\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "# Outer loop optimization (see MAML paper)\n",
        "@jit\n",
        "def step_outter(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "                opt_state_maml, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test):\n",
        "\n",
        "    grads = grad(step_inner_loss_mean, argnums=0)(params, C_bot, C_top, F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv, U_T_ic_intv, U_a_ic_intv, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "                                        F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test)\n",
        "    updates, opt_state_maml = optimizer_maml.update(grads, opt_state_maml, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, opt_state_maml"
      ],
      "metadata": {
        "id": "k1kkpiozb3Zy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate boundary, collocation and test points\n",
        "def data_generator(t_b, t_e, n_bc, n_f):\n",
        "\n",
        "  engine_bc = qmc.LatinHypercube(d=1)\n",
        "  sample_bc = engine_bc.random(n=n_bc)\n",
        "  t_interval = t_b + (t_e-t_b)*sample_bc\n",
        "  F_bct = np.hstack([np.ones(t_interval.shape)*(x.max()), t_interval])\n",
        "  F_bcb = np.hstack([np.ones(t_interval.shape)*(x.min()), t_interval])\n",
        "\n",
        "  x_lb, x_ub = x[1], x[-2]\n",
        "  t_lb, t_ub = t_b, t_e\n",
        "  lb = np.array([x_lb, t_lb])\n",
        "  ub = np.array([x_ub, t_ub])\n",
        "  engine = qmc.LatinHypercube(d=2)\n",
        "  sample = engine.random(n=n_f)\n",
        "  F_pde = lb + (ub-lb)*sample\n",
        "\n",
        "  F_intv = F[np.logical_and(F[:, 1]> t_b, F[:, 1]< t_e)]\n",
        "\n",
        "  return F_bcb, F_bct, F_pde, F_intv"
      ],
      "metadata": {
        "id": "Sq6aDtX0b3cq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training/test points for the time interval n for training the meta-learner\n",
        "def training_data(time_b, delta, n_ic, n_bc, n_f, n_ic_test, n_bc_test, n_f_test):\n",
        "\n",
        "  time_intv = time_b + delta\n",
        "\n",
        "  # Training\n",
        "  F_bcb_intv, F_bct_intv, F_pde_intv, F_intv = data_generator(time_b, time_intv, n_bc, n_f)\n",
        "  ic_x_indx = np.linspace(0, 1, n_ic)\n",
        "  ic_t_indx = np.ones(ic_x_indx.shape)*time_b\n",
        "  F_ic_intv = np.hstack([ic_x_indx.reshape([-1,1]), ic_t_indx.reshape([-1,1])])\n",
        "\n",
        "  # Test\n",
        "  F_bcb_test, F_bct_test, F_pde_test, F_test = data_generator(time_b, time_intv, n_bc_test, n_f_test)\n",
        "  ic_x_indx = np.linspace(0, 1, n_ic_test)\n",
        "  ic_t_indx = np.ones(ic_x_indx.shape)*time_b\n",
        "  F_ic_test = np.hstack([ic_x_indx.reshape([-1,1]), ic_t_indx.reshape([-1,1])])\n",
        "\n",
        "  return F_ic_intv, F_bcb_intv, F_bct_intv, F_pde_intv, F_intv, F_ic_test, F_bcb_test, F_bct_test, F_pde_test, F_test"
      ],
      "metadata": {
        "id": "Ka5wPP4mdWBy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate initial conditions for meta tasks\n",
        "def meta_inits(n_task, time_b, T_min, U_min, F_ic_intv, F_ic_test, params_meta):\n",
        "\n",
        "  U_T_ic_intv_meta = []\n",
        "  U_a_ic_intv_meta = []\n",
        "  U_T_ic_test_meta = []\n",
        "  U_a_ic_test_meta = []\n",
        "\n",
        "  for par_i in range(n_task):\n",
        "    if time_b == 0.0:\n",
        "\n",
        "      U_T_ic_intv = np.ones([F_ic_intv.shape[0], 1])*T_min\n",
        "      U_a_ic_intv = np.ones([F_ic_intv.shape[0], 1])*U_min\n",
        "      U_T_ic_test = np.ones([F_ic_test.shape[0], 1])*T_min\n",
        "      U_a_ic_test = np.ones([F_ic_test.shape[0], 1])*U_min\n",
        "\n",
        "    else:\n",
        "      U_T_ic_intv = u_pred_fn(params_meta[par_i], F_ic_intv[:,0], F_ic_intv[:,1])\n",
        "      U_a_ic_intv = u_pred_fn_a(params_meta[par_i], F_ic_intv[:,0], F_ic_intv[:,1])\n",
        "      U_T_ic_test =u_pred_fn(params_meta[par_i], F_ic_test[:,0], F_ic_test[:,1])\n",
        "      U_a_ic_test = u_pred_fn_a(params_meta[par_i], F_ic_test[:,0], F_ic_test[:,1])\n",
        "\n",
        "    U_T_ic_intv_meta.append(U_T_ic_intv)\n",
        "    U_a_ic_intv_meta.append(U_a_ic_intv)\n",
        "    U_T_ic_test_meta.append(U_T_ic_test)\n",
        "    U_a_ic_test_meta.append(U_a_ic_test)\n",
        "\n",
        "  U_T_ic_intv_meta_ar = np.array(U_T_ic_intv_meta)\n",
        "  U_a_ic_intv_meta_ar = np.array(U_a_ic_intv_meta)\n",
        "  U_T_ic_test_meta_ar = np.array(U_T_ic_test_meta)\n",
        "  U_a_ic_test_meta_ar = np.array(U_a_ic_test_meta)\n",
        "\n",
        "  return U_T_ic_intv_meta_ar, U_a_ic_intv_meta_ar, U_T_ic_test_meta_ar, U_a_ic_test_meta_ar"
      ],
      "metadata": {
        "id": "w0YqGy3ZdWJS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta learner training\n",
        "def meta_train(nIter, params, opt_state_maml, shuffle = False, ignore_pde = False):\n",
        "\n",
        "  # Ignore PDE/ODE loss terms for warming up (only for the first time interval)\n",
        "  # See paper's remarks: https://arxiv.org/abs/2308.06447\n",
        "  pbar = trange(10000)\n",
        "  if ignore_pde:\n",
        "    for it in pbar:\n",
        "      # Set pde weights to 0\n",
        "      params, opt_state_maml = step_outter(params, C_tasks[:,0],  C_tasks[:,1], F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv,\n",
        "                                    U_T_ic_intv_meta_ar, U_a_ic_intv_meta_ar, w_ic_T, w_ic_a, w_bcb, w_bct, 0.0,\n",
        "                                    opt_state_maml, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test_meta_ar, U_a_ic_test_meta_ar)\n",
        "\n",
        "  pbar = trange(nIter)\n",
        "  for it in pbar:\n",
        "\n",
        "    params, opt_state_maml = step_outter(params, C_tasks[:,0],  C_tasks[:,1], F_pde_intv, F_ic_intv, F_bcb_intv, F_bct_intv,\n",
        "                                        U_T_ic_intv_meta_ar, U_a_ic_intv_meta_ar, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde,\n",
        "                                        opt_state_maml, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test_meta_ar, U_a_ic_test_meta_ar)\n",
        "\n",
        "    # Evaluate progress on one of the support set's task\n",
        "    if it % 100 == 0 and it > 0:\n",
        "        loss_value = loss(params, C_tasks[0,0],  C_tasks[0,1], F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test_meta_ar[0],\n",
        "                          U_a_ic_test_meta_ar[0], w_ic_T, w_ic_a, w_bcb, w_bct, w_pde)\n",
        "        loss_ics_T_value = loss_ics_T(params, F_ic_test, U_T_ic_test_meta_ar[0])\n",
        "        loss_ics_a_value = loss_ics_a(params, F_ic_test, U_a_ic_test_meta_ar[0])\n",
        "        loss_bcb_value = loss_bcb(params, C_tasks[:,0], F_bcb_test)\n",
        "        loss_bct_value = loss_bct(params, C_tasks[:,1],  F_bct_test)\n",
        "        loss_pde_value = loss_pde(params, F_pde_test)\n",
        "        loss_ode_value = loss_ode(params, F_pde_test)\n",
        "\n",
        "        # Make shuffle True if you want to shuffle training points for training improvement\n",
        "        #if shuffle:\n",
        "        #  F_bcb_intv, F_bct_intv, F_pde_intv, _ = data_generator(time_b, time_intv, n_bc, n_f)\n",
        "        #  F_bcb_test, F_bct_test, F_pde_test, _ = data_generator(time_b, time_intv, n_bc_test, n_f_test)\n",
        "\n",
        "        pbar.set_postfix({'Loss': loss_value,\n",
        "                  'loss_ics_T' : loss_ics_T_value,\n",
        "                  'loss_ics_a' : loss_ics_a_value,\n",
        "                  'loss_bcb' : loss_bcb_value,\n",
        "                  'loss_bct' : loss_bct_value,\n",
        "                  'loss_pde':  loss_pde_value,\n",
        "                  'loss_ode': loss_ode_value,\n",
        "                  'w_ic_T' : w_ic_T,\n",
        "                  'w_ic_a' : w_ic_a,\n",
        "                  'w_bcb' : w_bcb,\n",
        "                  'w_bct' : w_bct,})\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "u6-abSK_jUCM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fine-tune meta-learner on each support set task to obtain the initial\n",
        "# conditions for the next interval\n",
        "nIter = 5000\n",
        "def meta_task_training(params, params_meta, C_tasks, nIter):\n",
        "# Training tasks individually to obtain the initial condition of the next time interval\n",
        "\n",
        "  params_meta = []\n",
        "  for i in range(n_task):\n",
        "    params_sub = params\n",
        "\n",
        "    C_bot, C_top = C_tasks[i,:]\n",
        "\n",
        "    scheduler = optax.exponential_decay(init_value=1e-5, transition_steps=5000, decay_rate=0.9)\n",
        "    optimizer = optax.adam(learning_rate=scheduler)\n",
        "    opt_state = optimizer.init(params_sub)\n",
        "\n",
        "    F_ic_test, F_bcb_test, F_bct_test, F_pde_test, F_test\n",
        "    if time_b == 0.0:\n",
        "      U_T_ic_intv = np.ones([F_ic_test.shape[0], 1])*T_min\n",
        "      U_a_ic_intv = np.ones([F_ic_test.shape[0], 1])*U_min\n",
        "    else:\n",
        "      U_T_ic_test =u_pred_fn(params_meta[i], F_ic_test[:,0], F_ic_test[:,1])\n",
        "      U_a_ic_test = u_pred_fn_a(params_meta[i], F_ic_test[:,0], F_ic_test[:,1])\n",
        "\n",
        "    print('Training task: ', str(i))\n",
        "\n",
        "    # Main training loop\n",
        "    pbar = trange(nIter)\n",
        "    for it in pbar:\n",
        "\n",
        "        # Finetuning\n",
        "        params_sub, opt_state = step_optax(params_sub, opt_state, C_bot, C_top, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test,\n",
        "                                           w_ic_T, w_ic_a, w_bcb, w_bct, w_pde)\n",
        "\n",
        "        if it % 50 == 0 and it > 0:\n",
        "            loss_value = loss(params_sub, C_bot, C_top, F_pde_test, F_ic_test, F_bcb_test, F_bct_test, U_T_ic_test, U_a_ic_test, w_ic_T, w_ic_a, w_bcb, w_bct, w_pde)\n",
        "            loss_ics_T_value = loss_ics_T(params_sub, F_ic_intv, U_T_ic_intv)\n",
        "            loss_ics_a_value = loss_ics_a(params_sub, F_ic_intv, U_a_ic_intv)\n",
        "            loss_bcb_value = loss_bcb(params_sub, C_bot, F_bcb_intv)\n",
        "            loss_bct_value = loss_bct(params_sub,C_top,  F_bct_intv)\n",
        "            loss_pde_value = loss_pde(params_sub, F_pde_intv)\n",
        "            loss_ode_value = loss_ode(params_sub, F_pde_intv)\n",
        "\n",
        "            pbar.set_postfix({'Loss': loss_value,\n",
        "                      'loss_ics_T' : loss_ics_T_value,\n",
        "                      'loss_ics_a' : loss_ics_a_value,\n",
        "                      'loss_bcb' : loss_bcb_value,\n",
        "                      'loss_bct' : loss_bct_value,\n",
        "                      'loss_pde':  loss_pde_value,\n",
        "                      'loss_ode': loss_ode_value,\n",
        "                      'HTC_top' : C_top,\n",
        "                      'HTC_bot' : C_bot})\n",
        "\n",
        "    params_meta.append(params_sub)\n",
        "\n",
        "  return params_meta"
      ],
      "metadata": {
        "id": "p0UNQS2F6ym3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training requirements\n",
        "# 1) initialize the neural network\n",
        "rng = jax.random.PRNGKey(0)\n",
        "x_init = jax.random.normal(rng, (20, 2))\n",
        "model = Net(features=2)\n",
        "params = model.init(rng, x_init)\n",
        "out = model.apply(params, x_init)\n",
        "\n",
        "# 2) Define the support set\n",
        "# Randomly select tasks from the defined task distribution (here, [0.18, 0.58])\n",
        "n_task = 20\n",
        "lb_meta = np.array([0.18, 0.18])\n",
        "ub_meta = np.array([0.58, 0.58])\n",
        "engine_meta = qmc.LatinHypercube(d=2, seed = 42)\n",
        "sample_meta = engine_meta.random(n=n_task)\n",
        "C_tasks = lb_meta + (ub_meta-lb_meta)*sample_meta\n",
        "\n",
        "# 3) Define weight hyperparameters\n",
        "w_ic_T = 100 # Temperature initial condition weight\n",
        "w_ic_a = 100 # DoC initial condition weight\n",
        "w_bcb = 1 # Bottom boundary condition weight\n",
        "w_bct = 1 # Top boundary condition weight\n",
        "w_pde = 1 # PDE/ODE weight\n",
        "\n",
        "# 4) Define time intervals (sequential learning)\n",
        "time_b_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9] # Intervals' starting point (normalized time domain)\n",
        "delta_list = [0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1] # Intervals' duration\n",
        "\n",
        "# 5) Training parameters\n",
        "nIter = 100000\n",
        "nIter_tasks = 100\n",
        "n_ic, n_bc, n_f = 200, 200, 2000 # training IC, BC and collocation points count\n",
        "n_ic_test, n_bc_test, n_f_test = 20, 20, 200 # test IC, BC and collocation points count\n",
        "params_meta = [] # initialize meta-task parameters for time interval 1\n",
        "\n",
        "# 6) Log\n",
        "meta_params_log = []\n",
        "meta_tasks_params_log = []"
      ],
      "metadata": {
        "id": "sEgN6IQWzcwJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(time_b_list)):\n",
        "\n",
        "  # Determine the time interval specifications\n",
        "  time_b = time_b_list[i]\n",
        "  delta = delta_list[i]\n",
        "  print('---------------------------------')\n",
        "  print('Time interval starts at:', time_b)\n",
        "  print('Time interval duration:', delta)\n",
        "  print('---------------------------------')\n",
        "  print('Training meta-learner', i+1)\n",
        "\n",
        "  # Specify training/test data (IC, BC and collocation) for meta learner\n",
        "  F_ic_intv, F_bcb_intv, F_bct_intv, F_pde_intv, F_intv, F_ic_test, F_bcb_test, F_bct_test, F_pde_test, F_test = training_data(time_b, delta, n_ic, n_bc, n_f, n_ic_test, n_bc_test, n_f_test)\n",
        "\n",
        "  # Initialize the support set tasks initial conditions (required for calculating the IC loss)\n",
        "  U_T_ic_intv_meta_ar, U_a_ic_intv_meta_ar, U_T_ic_test_meta_ar, U_a_ic_test_meta_ar = meta_inits(n_task, time_b, T_min, U_min, F_ic_intv, F_ic_test, params_meta)\n",
        "\n",
        "  # Define optimizer and decay rate for training the meta-learner\n",
        "  scheduler = optax.exponential_decay(init_value=1e-5, transition_steps=5000, decay_rate=0.9)\n",
        "  optimizer_maml = optax.adam(learning_rate=scheduler)\n",
        "  opt_state_maml = optimizer_maml.init(params)\n",
        "\n",
        "  # Training meta-learner for each time interval sequentially\n",
        "  if i < 0.1:\n",
        "    ignore_pde = True\n",
        "  else:\n",
        "    ignore_pde = False\n",
        "  params = meta_train(nIter, params, opt_state_maml, shuffle = False, ignore_pde = ignore_pde)\n",
        "  meta_params_log.append(params)\n",
        "\n",
        "  print('Training supprot set tasks to obtain IC for interval', i+2)\n",
        "\n",
        "  # Fine-tune meta-learner on each support set task to obtain the initial\n",
        "  # conditions for the next interval\n",
        "  params_meta = meta_task_training(params, params_meta, C_tasks, nIter_tasks)\n",
        "  meta_tasks_params_log.append(params_meta)\n"
      ],
      "metadata": {
        "id": "h0fOyB9Xyh6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}